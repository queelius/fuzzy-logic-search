{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I load the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class AlgebraicSearchEngine:\n",
    "    def __init__(self,\n",
    "                 docs,\n",
    "                 stemmer=PorterStemmer(),\n",
    "                 vectorizer=TfidfVectorizer(),\n",
    "                 stopwords=stopwords.words('english')):\n",
    "\n",
    "        self.stemmer = stemmer\n",
    "        self.vectorizer = vectorizer\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "        # Perform stemming on the documents\n",
    "        proc_docs = []\n",
    "        for doc in docs:\n",
    "            proc_tokens = self.process_doc(doc)\n",
    "            proc_doc = ' '.join(proc_tokens)\n",
    "\n",
    "            # Add the stemmed document to the list of stemmed documents\n",
    "            proc_docs.append(proc_doc)\n",
    "\n",
    "        self.docs = docs\n",
    "        self.proc_docs = proc_docs\n",
    "        self.proc_doc_vectors = self.vectorizer.fit_transform(proc_docs).toarray()\n",
    "\n",
    "    def process_doc(self, doc):\n",
    "        # remove punctuation\n",
    "        doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "\n",
    "        # lower-case\n",
    "        doc = doc.lower()\n",
    "\n",
    "        # split the string into words\n",
    "        words = doc.split()\n",
    "\n",
    "        # remove stopwords\n",
    "        words = [word for word in words if word not in self.stopwords]\n",
    "\n",
    "        # stem each word and join them back into a string\n",
    "        return [self.stemmer.stem(word) for word in words]\n",
    "\n",
    "    def recursive_search(self, tokens):\n",
    "        operator = tokens.pop(0).upper()\n",
    "\n",
    "        if operator not in ['AND', 'OR', 'NOT']:\n",
    "            raise ValueError(f\"Invalid operator {operator}\")\n",
    "        \n",
    "        operands = []\n",
    "        while tokens[0] != ')':\n",
    "            if tokens[0] == '(':\n",
    "                tokens.pop(0)  # Remove '('\n",
    "                operands.append(self.recursive_search(tokens))\n",
    "            else:\n",
    "                term_vec = self.vectorizer.transform([tokens.pop(0)]).toarray()[0]\n",
    "                term_scores = self.proc_doc_vectors.dot(term_vec)\n",
    "                operands.append(term_scores)\n",
    "\n",
    "        \n",
    "\n",
    "        tokens.pop(0)  # Remove ')'\n",
    "        result = None\n",
    "        if operator == 'AND':\n",
    "            result = np.min(np.array(operands), axis=0)\n",
    "        elif operator == 'OR':\n",
    "            result = np.max(np.array(operands), axis=0)\n",
    "        elif operator == 'NOT':\n",
    "            if len(operands) != 1:\n",
    "                raise ValueError(\"NOT operator can only have one operand\")\n",
    "            result = 1 - operands[0]\n",
    "        return result\n",
    "    \n",
    "    def process_query(self, query):\n",
    "        tokens = re.findall(r'\\b\\w+\\b|\\(|\\)', query)\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def search(self, query):\n",
    "        tokens = self.process_query(query)\n",
    "        if tokens[0] != '(':\n",
    "            raise ValueError(\"Invalid query\")\n",
    "        \n",
    "        tokens = tokens[1:]\n",
    "\n",
    "        scores = self.recursive_search(tokens)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"The cat in the hat\",\n",
    "        \"This is just a document with no other purpose than to show how the search engine works.\",\n",
    "        \"A dog and his boy.\",\n",
    "        \"A boy jumps up and down.\",\n",
    "        \"The cats are out of the bag.\",\n",
    "        \"Dogs and cats, living together.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Cats, cats, cats, cats, cats, and maybe a dog!\",\n",
    "        \"The dog did not bite the cat.\",\n",
    "        \"quick dog cat\",\n",
    "        \"a quick dog bite a cat\",\n",
    "        \"dog cat\",\n",
    "        \"quick dog\",\n",
    "        \"Dog, dogs, dogs, dogs, dogs! And maybe a cat.\",\n",
    "        \"Dog, dogs, dogs! And maybe a cat.\",\n",
    "        \"Okay, now is the time, for all the good men, to come to the aid of their country.\",\n",
    "        \"Cat cat cat cat cat cat cats cats cats!\",\n",
    "        \"test\"]\n",
    "boolean_search_engine = AlgebraicSearchEngine(docs=docs, vectorizer=CountVectorizer(binary=True))\n",
    "fuzzy_search_engine =   AlgebraicSearchEngine(docs=docs, vectorizer=TfidfVectorizer())\n",
    "#query = \"(AND (AND cat (NOT bite) dog) (AND dog quick) (NOT (AND boy jump)))\"\n",
    "query = \"(AND cat dog)\"\n",
    "print(f\"Query: {query}\")\n",
    "results = boolean_search_engine.search(query)\n",
    "# pretty print the results\n",
    "for i, doc in enumerate(boolean_search_engine.docs):\n",
    "    print(f\"Document {i+1}: {doc} => {results[i]}\")\n",
    "\n",
    "print(\"---\")\n",
    "results = fuzzy_search_engine.search(query)\n",
    "# pretty print the results\n",
    "for i, doc in enumerate(fuzzy_search_engine.docs):\n",
    "    print(f\"Document {i+1}: {doc} => {results[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
