{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class AlgebraicSearchEngine:\n",
    "    def __init__(self,\n",
    "                 docs : list[str],\n",
    "                 stemmer=PorterStemmer(),\n",
    "                 vectorizer=TfidfVectorizer(),\n",
    "                 stopwords=stopwords.words('english')):\n",
    "\n",
    "        self.stemmer = stemmer\n",
    "        self.vectorizer = vectorizer\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "        proc_docs = []\n",
    "        for doc in docs:\n",
    "            proc_tokens = self.process_doc(doc)\n",
    "            proc_doc = ' '.join(proc_tokens)\n",
    "            # Add the stemmed document to the list of stemmed documents\n",
    "            proc_docs.append(proc_doc)\n",
    "\n",
    "        # Store the original and processed documents\n",
    "        self.docs = docs\n",
    "        # Store the processed document vectors\n",
    "        self.vecs = self.vectorizer.fit_transform(proc_docs).toarray()\n",
    "\n",
    "    def process_query(self, query: str) -> list[str]:\n",
    "        tokens = re.findall(r'\\b\\w+\\b|\\(|\\)', query)\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def process_doc(self, doc : str) -> list[str]:\n",
    "        # remove punctuation\n",
    "        doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "        # lower-case\n",
    "        doc = doc.lower()\n",
    "        # split the string into words\n",
    "        words = doc.split()\n",
    "        # remove stopwords\n",
    "        words = [word for word in words if word not in self.stopwords]\n",
    "        # stem each word and join them back into a string\n",
    "        words = [self.stemmer.stem(word) for word in words]\n",
    "        return words\n",
    "\n",
    "    def recursive_search(self, tokens):\n",
    "        operator = tokens.pop(0).upper()\n",
    "\n",
    "        if operator not in ['AND', 'OR', 'NOT']:\n",
    "            raise ValueError(f\"Invalid operator {operator}\")\n",
    "        \n",
    "        operands = []\n",
    "        while tokens[0] != ')':\n",
    "            if tokens[0] == '(':\n",
    "                tokens.pop(0)  # Remove '('\n",
    "                operands.append(self.recursive_search(tokens))\n",
    "            else:\n",
    "                term_vec = self.vectorizer.transform([tokens.pop(0)]).toarray()[0]\n",
    "                term_scores = self.vecs.dot(term_vec)\n",
    "                operands.append(term_scores)\n",
    "\n",
    "        tokens.pop(0)  # Remove ')'\n",
    "        result = None\n",
    "        if operator == 'AND':\n",
    "            result = np.min(np.array(operands), axis=0)\n",
    "        elif operator == 'OR':\n",
    "            result = np.max(np.array(operands), axis=0)\n",
    "        elif operator == 'NOT':\n",
    "            if len(operands) != 1:\n",
    "                raise ValueError(\"NOT operator can only have one operand\")\n",
    "            #result = 1 - operands[0]\n",
    "            result = operands[0]**0.5\n",
    "        return result\n",
    "    \n",
    "    def search(self, query):\n",
    "        tokens = self.process_query(query)\n",
    "        if tokens[0] != '(':\n",
    "            raise ValueError(\"Invalid query\")\n",
    "        \n",
    "        tokens = tokens[1:]\n",
    "\n",
    "        scores = self.recursive_search(tokens)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'cat', 'live', 'togeth']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"The cat in the hat\",\n",
    "        \"This is just a document with no other purpose than to show how the search engine works.\",\n",
    "        \"A dog and his boy.\",\n",
    "        \"A boy jumps up and down.\",\n",
    "        \"The cats are out of the bag.\",\n",
    "        \"Dogs and cats, living together.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Cats, cats, cats, cats, cats, and maybe a dog!\",\n",
    "        \"The dog did not bite the cat.\",\n",
    "        \"quick dog cat\",\n",
    "        \"a quick dog bite a cat\",\n",
    "        \"dog cat\",\n",
    "        \"quick dog\",\n",
    "        \"Dog, dogs, dogs, dogs, dogs! And maybe a cat.\",\n",
    "        \"Dog, dogs, dogs! And maybe a cat.\",\n",
    "        \"Okay, now is the time, for all the good men, to come to the aid of their country.\",\n",
    "        \"Cat cat cat cat cat cat cats cats cats!\",\n",
    "        \"cat dog quick\",\n",
    "        \"test\"]\n",
    "boolean_search_engine = AlgebraicSearchEngine(\n",
    "    docs=docs,\n",
    "    vectorizer=CountVectorizer(binary=True))\n",
    "\n",
    "boolean_search_engine.process_doc(\"Dogs and cats, living together!!!\")\n",
    "# Document: \"Dogs and cats, living together!!!\",\n",
    "#   boolean_search_engine.process_doc(\"Dogs and cats, living together!!!\")\n",
    "#   -> pre-processed: ['dogs', 'and', 'cats', 'living', 'together']\n",
    "#   -> stop-words removed: ['dogs', 'cats', 'living', 'together']\n",
    "#   -> stemmed words: ['dog', 'cat', 'live', 'togeth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show how a query is processed. Recall the query permits `AND`, `OR`, and\n",
    "`NOT`, which is sufficient to implement a Boolean algebra. In particular, this\n",
    "means that given a query, a subset of the documents is denoted by the query.\n",
    "\n",
    "We imagine that the Boolean algebra is over the powerset of the words in the\n",
    "all the documents. A query is a special way of specifying a subset of the\n",
    "powerset of the words in the documents. The subset is specified by the\n",
    "occurrence of the words in the query. For example, the query `A AND B` denotes\n",
    "the set of documents that contain both `A` and `B`. The query `A OR B` denotes\n",
    "the set of documents that contain either `A` or `B`. The query `NOT A` denotes\n",
    "the set of documents that do not contain `A`. We may combine these to form\n",
    "complex queries. For example, the query `A AND (B OR NOT C) AND NOT D` denotes the\n",
    "set of documents that contain `A` and either `B` or not `C` and do not contain\n",
    "`D`.\n",
    "\n",
    "A document is said to be relevant to a query if it is in the subset denoted by\n",
    "the query. The goal of the search engine is to return the documents that are\n",
    "relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: (AND cat (NOT bite) dog)\n",
      "Processed Query: ['(', 'and', 'cat', '(', 'not', 'bite', ')', 'dog', ')']\n"
     ]
    }
   ],
   "source": [
    "query = \"(AND cat (NOT bite) dog)\"# (AND dog quick) (NOT (AND boy jump)))\"\n",
    "#query = \"(AND cat (NOT dog))\"\n",
    "print(f\"Query: {query}\")\n",
    "# Query: (AND cat (NOT (OR dog men))\n",
    "print(f'Processed Query: {boolean_search_engine.process_query(query)}')\n",
    "# Processed Query: ['(', 'and', 'cat', '(', 'not', '(', 'or', 'dog', 'men', ')', ')']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results=array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]) results.shape=(19,)\n",
      "Document 0: The cat in the hat => (8, 1.0)\n",
      "Document 1: This is just a document with no other purpose than to show how the search engine works. => (10, 1.0)\n",
      "Document 2: A dog and his boy. => (0, 0.0)\n",
      "Document 3: A boy jumps up and down. => (1, 0.0)\n",
      "Document 4: The cats are out of the bag. => (2, 0.0)\n",
      "Document 5: Dogs and cats, living together. => (3, 0.0)\n",
      "Document 6: The quick brown fox jumps over the lazy dog. => (4, 0.0)\n",
      "Document 7: Cats, cats, cats, cats, cats, and maybe a dog! => (5, 0.0)\n",
      "Document 8: The dog did not bite the cat. => (6, 0.0)\n",
      "Document 9: quick dog cat => (7, 0.0)\n",
      "Document 10: a quick dog bite a cat => (9, 0.0)\n",
      "Document 11: dog cat => (11, 0.0)\n",
      "Document 12: quick dog => (12, 0.0)\n",
      "Document 13: Dog, dogs, dogs, dogs, dogs! And maybe a cat. => (13, 0.0)\n",
      "Document 14: Dog, dogs, dogs! And maybe a cat. => (14, 0.0)\n",
      "Document 15: Okay, now is the time, for all the good men, to come to the aid of their country. => (15, 0.0)\n",
      "Document 16: Cat cat cat cat cat cat cats cats cats! => (16, 0.0)\n",
      "Document 17: cat dog quick => (17, 0.0)\n",
      "Document 18: test => (18, 0.0)\n"
     ]
    }
   ],
   "source": [
    "query = \"(AND cat (NOT bite) dog)\"# (AND dog quick) (NOT (AND boy jump)))\"\n",
    "#query = \"(AND cat (NOT dog))\"\n",
    "print(f\"Query: {query}\")\n",
    "# Query: (AND cat (NOT (OR dog men))\n",
    "print(f'Processed Query: {boolean_search_engine.process_query(query)}')\n",
    "# Processed Query: ['(', 'and', 'cat', '(', 'not', '(', 'or', 'dog', 'men', ')', ')']\n",
    "results = boolean_search_engine.search(query)\n",
    "print(f\"{results=} {results.shape=}\")\n",
    "# sort results by score\n",
    "results = sorted(enumerate(results), key=lambda x: x[1], reverse=True)\n",
    "# pretty print the results\n",
    "for i, doc in enumerate(boolean_search_engine.docs):\n",
    "    print(f\"Document {i}: {doc} => {results[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use TF-IDF to score the words in the text. We will use the TfidfVectorizer from the scikit-learn library to convert the text into a matrix of TF-IDF features. We will then use the cosine similarity to find the similarity between the text and the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: The cat in the hat => 0.0\n",
      "Document 2: This is just a document with no other purpose than to show how the search engine works. => 0.0\n",
      "Document 3: A dog and his boy. => 0.0\n",
      "Document 4: A boy jumps up and down. => 0.0\n",
      "Document 5: The cats are out of the bag. => 0.0\n",
      "Document 6: Dogs and cats, living together. => 0.28958595184220776\n",
      "Document 7: The quick brown fox jumps over the lazy dog. => 0.0\n",
      "Document 8: Cats, cats, cats, cats, cats, and maybe a dog! => 0.43065672355472245\n",
      "Document 9: The dog did not bite the cat. => 0.4151629224275721\n",
      "Document 10: quick dog cat => 0.46832112601474213\n",
      "Document 11: a quick dog bite a cat => 0.3458313340667656\n",
      "Document 12: dog cat => 0.7071067811865476\n",
      "Document 13: quick dog => 0.0\n",
      "Document 14: Dog, dogs, dogs, dogs, dogs! And maybe a cat. => 0.18546521354288864\n",
      "Document 15: Dog, dogs, dogs! And maybe a cat. => 0.2765851124973335\n",
      "Document 16: Okay, now is the time, for all the good men, to come to the aid of their country. => 0.0\n",
      "Document 17: Cat cat cat cat cat cat cats cats cats! => 0.0\n",
      "Document 18: test => 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "fuzzy_search_engine =   AlgebraicSearchEngine(docs=docs, vectorizer=TfidfVectorizer())\n",
    "results = fuzzy_search_engine.search(query)\n",
    "# pretty print the results\n",
    "for i, doc in enumerate(fuzzy_search_engine.docs):\n",
    "    print(f\"Document {i+1}: {doc} => {results[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-cpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
