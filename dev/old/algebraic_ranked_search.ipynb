{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class AlgebraicSearchEngine:\n",
    "    def __init__(self,\n",
    "                 docs : list[str],\n",
    "                 stemmer=PorterStemmer(),\n",
    "                 vectorizer=TfidfVectorizer(),\n",
    "                 stopwords=stopwords.words('english')):\n",
    "\n",
    "        self.stemmer = stemmer\n",
    "        self.vectorizer = vectorizer\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "        proc_docs = []\n",
    "        for doc in docs:\n",
    "            proc_tokens = self.process_doc(doc)\n",
    "            proc_doc = ' '.join(proc_tokens)\n",
    "            # Add the stemmed document to the list of stemmed documents\n",
    "            proc_docs.append(proc_doc)\n",
    "\n",
    "        # Store the original and processed documents\n",
    "        self.docs = docs\n",
    "        # Store the processed document vectors\n",
    "        self.vecs = self.vectorizer.fit_transform(proc_docs).toarray()\n",
    "\n",
    "    def process_query(self, query: str) -> list[str]:\n",
    "        tokens = re.findall(r'\\b\\w+\\b|\\(|\\)', query)\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def process_doc(self, doc : str) -> list[str]:\n",
    "        # remove punctuation\n",
    "        doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "        # lower-case\n",
    "        doc = doc.lower()\n",
    "        # split the string into words\n",
    "        words = doc.split()\n",
    "        # remove stopwords\n",
    "        words = [word for word in words if word not in self.stopwords]\n",
    "        # stem each word and join them back into a string\n",
    "        words = [self.stemmer.stem(word) for word in words]\n",
    "        return words\n",
    "\n",
    "    def recursive_search(self, tokens):\n",
    "        operator = tokens.pop(0).upper()\n",
    "\n",
    "        if operator not in ['AND', 'OR', 'NOT']:\n",
    "            raise ValueError(f\"Invalid operator {operator}\")\n",
    "        \n",
    "        operands = []\n",
    "        while tokens[0] != ')':\n",
    "            if tokens[0] == '(':\n",
    "                tokens.pop(0)  # Remove '('\n",
    "                operands.append(self.recursive_search(tokens))\n",
    "            else:\n",
    "                term_vec = self.vectorizer.transform([tokens.pop(0)]).toarray()[0]\n",
    "                term_scores = self.vecs.dot(term_vec)\n",
    "                operands.append(term_scores)\n",
    "\n",
    "        tokens.pop(0)  # Remove ')'\n",
    "        result = None\n",
    "        #print(operands)\n",
    "        if operator == 'AND':\n",
    "            result = np.min(np.array(operands), axis=0)\n",
    "            print(f\"AND: {np.round(result, decimals=2)}\")\n",
    "        elif operator == 'OR':\n",
    "            result = np.max(np.array(operands), axis=0)\n",
    "            print(f\" OR: {np.round(result, decimals=2)}\")\n",
    "        elif operator == 'NOT':\n",
    "            if len(operands) != 1:\n",
    "                raise ValueError(\"NOT operator can only have one operand\")\n",
    "            \n",
    "            result = 1 - operands[0]\n",
    "            print(f\"NOT: {np.round(result, decimals=2)}\")\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def search(self, query) -> np.ndarray:\n",
    "        tokens = self.process_query(query)\n",
    "        if tokens[0] != '(':\n",
    "            raise ValueError(\"Invalid query\")\n",
    "        \n",
    "        tokens = tokens[1:]\n",
    "\n",
    "        scores = self.recursive_search(tokens)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'cat', 'live', 'togeth']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"The cat in the hat\",\n",
    "        \"This is just a document with no other purpose than to show how the search engine works.\",\n",
    "        \"A dog and his boy.\",\n",
    "        \"A boy jumps up and down.\",\n",
    "        \"The cats are out of the bag.\",\n",
    "        \"Dogs and cats, living together.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Cats, cats, cats, cats, cats, and maybe a dog!\",\n",
    "        \"The dog did not bite the cat.\",\n",
    "        \"The quick brown dog jumps over the lazy cat.\",\n",
    "        \"The super fast brown dog jumps over the lazy cat.\",\n",
    "        \"The dog brown dog jumps over the lazy cat.\",\n",
    "        \"a quick dog bite a cat\",\n",
    "        \"dog cat\",\n",
    "        \"quick dog\",\n",
    "        \"Dog, dogs, dogs, dogs, dogs! And maybe a cat.\",\n",
    "        \"Dog, dogs, dogs! And maybe a cat.\",\n",
    "        \"Okay, now is the time, for all the good men, to come to the aid of their country.\",\n",
    "        \"Cat cat cat cat cat cat cats cats cats!\",\n",
    "        \"test\"]\n",
    "boolean_search_engine = AlgebraicSearchEngine(\n",
    "    docs=docs,\n",
    "    vectorizer=CountVectorizer(binary=True))\n",
    "\n",
    "boolean_search_engine.process_doc(\"Dogs and cats, living together!!!\")\n",
    "# Document: \"Dogs and cats, living together!!!\",\n",
    "#   boolean_search_engine.process_doc(\"Dogs and cats, living together!!!\")\n",
    "#   -> pre-processed: ['dogs', 'and', 'cats', 'living', 'together']\n",
    "#   -> stop-words removed: ['dogs', 'cats', 'living', 'together']\n",
    "#   -> stemmed words: ['dog', 'cat', 'live', 'togeth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show how a query is processed. Recall the query permits `AND`, `OR`, and\n",
    "`NOT`, which is sufficient to implement a Boolean algebra. In particular, this\n",
    "means that given a query, a subset of the documents is denoted by the query.\n",
    "\n",
    "We imagine that the Boolean algebra is over the powerset of the words in the\n",
    "all the documents. A query is a special way of specifying a subset of the\n",
    "powerset of the words in the documents. The subset is specified by the\n",
    "occurrence of the words in the query. For example, the query `A AND B` denotes\n",
    "the set of documents that contain both `A` and `B`. The query `A OR B` denotes\n",
    "the set of documents that contain either `A` or `B`. The query `NOT A` denotes\n",
    "the set of documents that do not contain `A`. We may combine these to form\n",
    "complex queries. For example, the query `A AND (B OR NOT C) AND NOT D` denotes the\n",
    "set of documents that contain `A` and either `B` or not `C` and do not contain\n",
    "`D`.\n",
    "\n",
    "A document is said to be relevant to a query if it is in the subset denoted by\n",
    "the query. The goal of the search engine is to return the documents that are\n",
    "relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: (AND cat dog (NOT bite) (OR quick (AND super fast)))\n",
      "Processed query: ['(', 'and', 'cat', 'dog', '(', 'not', 'bite', ')', '(', 'or', 'quick', '(', 'and', 'super', 'fast', ')', ')', ')']\n",
      "Boolean search\n",
      "NOT: [1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1]\n",
      "AND: [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " OR: [0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0]\n",
      "AND: [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "query = \"(AND cat dog (NOT bite) (OR quick (AND super fast)))\"\n",
    "print(f\"Query: {query}\")\n",
    "# Query: (AND cat (NOT bite) dog (AND dog (OR quick fast)) (NOT boy))\n",
    "print(f'Processed query: {boolean_search_engine.process_query(query)}')\n",
    "# Processed query: ['(', 'and', 'cat', '(', 'not', 'bite', ')', 'dog', '(', 'and', 'dog', '(', 'or', 'quick', 'fast', ')', ')', '(', 'not', 'boy', ')', ')'\n",
    "\n",
    "print(\"Boolean search\")\n",
    "scores = boolean_search_engine.search(query)\n",
    "#print(f\"Scores: {scores}\")\n",
    "#for i, score in enumerate(scores):\n",
    "#    if score > 0:\n",
    "#        print(f\"Document {i}: {docs[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show the relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT: [1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1]\n",
      "AND: [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " OR: [0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0]\n",
      "AND: [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Document 9: The quick brown dog jumps over the lazy cat.\n",
      "Document 10: The super fast brown dog jumps over the lazy cat.\n"
     ]
    }
   ],
   "source": [
    "results = boolean_search_engine.search(query)\n",
    "# Only print the documents that have a score > 0\n",
    "res = [i for i in np.where(results > 0)[0]]\n",
    "# retrieve the original documents\n",
    "for i in res:\n",
    "    print(f\"Document {i}: {docs[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use TF-IDF to score the words in the text. We will use the TfidfVectorizer from the scikit-learn library to convert the text into a matrix of TF-IDF features. We will then use the cosine similarity to find the similarity between the text and the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT: [1.   1.   1.   1.   1.   1.   1.   1.   0.17 1.   1.   1.   0.32 1.\n",
      " 1.   1.   1.   1.   1.   1.  ]\n",
      "AND: [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.51 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.  ]\n",
      " OR: [0.   0.   0.   0.   0.   0.   0.41 0.   0.   0.47 0.51 0.   0.57 0.\n",
      " 0.87 0.   0.   0.   0.   0.  ]\n",
      "AND: [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.27 0.21 0.   0.32 0.\n",
      " 0.   0.   0.   0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "fuzzy_search_engine = AlgebraicSearchEngine(docs=docs, vectorizer=TfidfVectorizer())\n",
    "results = fuzzy_search_engine.search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the results, from most to least relevant. We'll put them in\n",
    "a list of tuples, `(index, score)`, where `index` is the index of the document\n",
    "in the list of documents and `score` is the score of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 12: a quick dog bite a cat (0.3161735956168409)\n",
      "Document 9: The quick brown dog jumps over the lazy cat. (0.2714157795710721)\n",
      "Document 10: The super fast brown dog jumps over the lazy cat. (0.2134685886816857)\n"
     ]
    }
   ],
   "source": [
    "# let's put the list in the form (i, score) and sort it\n",
    "res = sorted(enumerate(results), key=lambda x: x[1], reverse=True)\n",
    "for i in range(3):\n",
    "    i, score = res[i]\n",
    "    print(f\"Document {i}: {docs[i]} ({score})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
